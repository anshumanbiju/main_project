{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcribing audio...\n",
      "Subtitles saved to video.srt\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extracts the audio from a video file and saves it as a separate file.\n",
    "    \"\"\"\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    video.close()\n",
    "\n",
    "def generate_subtitles(video_path, model_name=\"base\"):\n",
    "    \"\"\"\n",
    "    Generates subtitles for a video using the Whisper model.\n",
    "    \"\"\"\n",
    "    # Check if the Whisper model is installed\n",
    "    try:\n",
    "        model = whisper.load_model(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Whisper model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Extract audio from video\n",
    "    audio_path = \"temp_audio.wav\"\n",
    "    extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "    # Transcribe audio\n",
    "    print(\"Transcribing audio...\")\n",
    "    result = model.transcribe(audio_path)\n",
    "\n",
    "    # Create SRT file\n",
    "    srt_path = os.path.splitext(video_path)[0] + \".srt\"\n",
    "    with open(srt_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "        for i, segment in enumerate(result[\"segments\"]):\n",
    "            # Write SRT segment\n",
    "            start = format_timestamp(segment[\"start\"])\n",
    "            end = format_timestamp(segment[\"end\"])\n",
    "            text = segment[\"text\"].strip()\n",
    "            srt_file.write(f\"{i + 1}\\n{start} --> {end}\\n{text}\\n\\n\")\n",
    "\n",
    "    print(f\"Subtitles saved to {srt_path}\")\n",
    "\n",
    "    # Clean up\n",
    "    os.remove(audio_path)\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    \"\"\"\n",
    "    Formats seconds into an SRT timestamp (HH:MM:SS,ms).\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{milliseconds:03}\"\n",
    "\n",
    "# Example usage\n",
    "video_path = \"video.mp4\"  # Replace with your video file path\n",
    "generate_subtitles(video_path, model_name=\"base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Subtitles saved to video2asrn_model_subtitles.srt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extracts the audio from a video file and saves it as a separate file.\n",
    "    \"\"\"\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    video.close()\n",
    "\n",
    "def generate_subtitles_from_model(video_path, model, processor):\n",
    "    \"\"\"\n",
    "    Generates subtitles for a video using the self-trained Wav2Vec2 model.\n",
    "    \"\"\"\n",
    "    # Extract audio from video\n",
    "    audio_path = \"temp_audio.wav\"\n",
    "    extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "    # Load the audio file using librosa\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)  # Ensure 16kHz sampling rate\n",
    "    os.remove(audio_path)  # Remove the temporary audio file\n",
    "\n",
    "    # Preprocess the audio and get model predictions\n",
    "    input_values = processor(audio, return_tensors=\"pt\", sampling_rate=sr).input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    # Decode the logits into transcription text\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    # Filter out the padding tokens (if any)\n",
    "    transcription = transcription.replace(\"<pad>\", \"\").strip()\n",
    "\n",
    "    # Split transcription into words or segments (adjust for your model output)\n",
    "    segments = segment_transcription(transcription, predicted_ids, processor, audio_length=len(audio)/sr)\n",
    "    \n",
    "    # Create SRT file with timestamps\n",
    "    srt_path = os.path.splitext(video_path)[0] + \"asrn_model_subtitles.srt\"\n",
    "    with open(srt_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "        for i, segment in enumerate(segments):\n",
    "            start = format_timestamp(segment['start_time'])\n",
    "            end = format_timestamp(segment['end_time'])\n",
    "            text = segment['text'].strip()\n",
    "            srt_file.write(f\"{i + 1}\\n{start} --> {end}\\n{text}\\n\\n\")\n",
    "\n",
    "    print(f\"Subtitles saved to {srt_path}\")\n",
    "\n",
    "def segment_transcription(transcription, predicted_ids, processor, audio_length, window_size=10, stride=5):\n",
    "    \"\"\"\n",
    "    Segments the transcription text into segments with approximate timestamps.\n",
    "    This is an approximation and can be adjusted according to model output.\n",
    "    \"\"\"\n",
    "    tokens = processor.tokenizer.convert_ids_to_tokens(predicted_ids[0].tolist())\n",
    "    \n",
    "    # Remove padding tokens from the transcription\n",
    "    tokens = [token for token in tokens if token != \"<pad>\"]\n",
    "\n",
    "    segments = []\n",
    "    num_tokens = len(tokens)\n",
    "    start_time = 0\n",
    "    # Calculate time per token (based on total audio length and number of tokens)\n",
    "    time_per_token = audio_length / num_tokens\n",
    "    \n",
    "    for i in range(0, len(tokens), window_size):\n",
    "        end_time = start_time + (window_size * time_per_token)  # Approximate based on window size\n",
    "        \n",
    "        # Ensure to avoid the last token being too long\n",
    "        segment_text = \" \".join(tokens[i:i + window_size]).strip()\n",
    "        \n",
    "        segments.append({\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": min(end_time, audio_length),  # Ensure the end time doesn't exceed audio length\n",
    "            \"text\": segment_text\n",
    "        })\n",
    "        \n",
    "        start_time = end_time\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    \"\"\"\n",
    "    Formats seconds into an SRT timestamp (HH:MM:SS,ms).\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{milliseconds:03}\"\n",
    "\n",
    "# Load your self-trained model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"./subtitle-generator\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./subtitle-generator\")\n",
    "\n",
    "# Example usage\n",
    "video_path = \"video2.mp4\"  # Replace with your video file path\n",
    "generate_subtitles_from_model(video_path, model, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\anshu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcribing audio using self-trained Wav2Vec2 model...\n",
      "Transcribing audio using Whisper model for correction...\n",
      "Subtitles saved to video2asr.srt\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extracts the audio from a video file and saves it as a separate file.\n",
    "    \"\"\"\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    video.close()\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    \"\"\"\n",
    "    Formats seconds into an SRT timestamp (HH:MM:SS,ms).\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{milliseconds:03}\"\n",
    "\n",
    "def generate_subtitles(video_path, model_path, processor_path, whisper_model_name=\"base\"):\n",
    "    \"\"\"\n",
    "    Generates subtitles for a video using the fine-tuned Wav2Vec2 model for transcription and the Whisper model for correction.\n",
    "    \"\"\"\n",
    "    # Load Whisper model for correction\n",
    "    try:\n",
    "        whisper_model = whisper.load_model(whisper_model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Whisper model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Load the fine-tuned Wav2Vec2 model and processor\n",
    "    asr_pipeline = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_path,\n",
    "        tokenizer=processor_path\n",
    "    )\n",
    "\n",
    "    # Extract audio from video\n",
    "    audio_path = \"temp_audio.wav\"\n",
    "    extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "    # Step 1: Transcribe audio with Wav2Vec2 model\n",
    "    print(\"Transcribing audio using self-trained Wav2Vec2 model...\")\n",
    "    result_wav2vec = asr_pipeline(audio_path)\n",
    "    wav2vec_transcript = result_wav2vec['text'].lower()  # Convert transcript to lowercase\n",
    "\n",
    "    # Step 2: Use Whisper model for better vocabulary and timestamp correction\n",
    "    print(\"Transcribing audio using Whisper model for correction...\")\n",
    "    result_whisper = whisper_model.transcribe(audio_path)\n",
    "\n",
    "    # Create SRT file path\n",
    "    srt_path = os.path.splitext(video_path)[0]+\"asr\" + \".srt\"\n",
    "\n",
    "    # Step 3: Generate subtitles using Whisper's improved segments\n",
    "    with open(srt_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "        for i, segment in enumerate(result_whisper[\"segments\"]):\n",
    "            # Write SRT segment using Whisper's improved timing and vocabulary\n",
    "            start = format_timestamp(segment[\"start\"])\n",
    "            end = format_timestamp(segment[\"end\"])\n",
    "            text = segment[\"text\"].strip()\n",
    "            srt_file.write(f\"{i + 1}\\n{start} --> {end}\\n{text}\\n\\n\")\n",
    "\n",
    "    print(f\"Subtitles saved to {srt_path}\")\n",
    "\n",
    "    # Clean up temporary audio file\n",
    "    os.remove(audio_path)\n",
    "\n",
    "# Example usage\n",
    "video_path = \"video2.mp4\"  # Replace with your video file path\n",
    "model_path = \"E:\\\\video_player\\\\subtitle-generator\"  # Path to the fine-tuned Wav2Vec2 model\n",
    "processor_path = \"E:\\\\video_player\\\\subtitle-generator\"  # Path to the saved processor\n",
    "generate_subtitles(video_path, model_path, processor_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\anshu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcribing audio using self-trained Wav2Vec2 model...\n",
      "\n",
      "Transcript from self-trained Wav2Vec2 model:\n",
      "so what's new mark how is your new job going to be honest i can't complain i really love the company that i am working for my co workers are all really friendly and helpful they really help me feel welcome it's a really energetic and fun atmosphere my boss is hilarius and he's really flexible really how so he allows me to come in when i want and make my own hours i can also leave early if i start early there is no real dress coat either i can wer genes and a tea shirt if i want i can even wear shorts in the summer wow it sounds really cool i can't stand wearing a suit every day which do you prefer working late or finishing early i prefer finishing early i really enjoy the morning i love getting up early and going for a rant there is nothing like watching the sunrise while drinking my morning coffee really i am opposite i love sleeping in iam most alert in the evenings i'm a real night owl well you know what they say the early bird catches the worm you know you could be right maybe i will try to go to bed a little earlier to night\n",
      "\n",
      "Transcribing audio using Whisper model for correction...\n",
      "\n",
      "Corrected subtitles from Whisper model:\n",
      "00:00:00,000 --> 00:00:17,760: So, what's new Mark? How is your new job going?\n",
      "00:00:17,760 --> 00:00:26,280: To be honest, I can't complain. I really love the company that I am working for.\n",
      "00:00:26,280 --> 00:00:33,280: My co-workers are all really friendly and helpful. They really help me feel welcome.\n",
      "00:00:33,280 --> 00:00:43,280: It's a really energetic and fun atmosphere. My boss is hilarious and he's really flexible.\n",
      "00:00:43,280 --> 00:00:47,280: Really? How so?\n",
      "00:00:47,280 --> 00:00:53,280: He allows me to come in when I want and make my own hours.\n",
      "00:00:53,280 --> 00:01:00,280: I can also leave early if I start early. There is no real dress code either.\n",
      "00:01:00,280 --> 00:01:08,280: I can wear jeans and a t-shirt if I want. I can even wear shorts in the summer.\n",
      "00:01:08,280 --> 00:01:16,280: Wow. It sounds really cool. I can't stand wearing a suit every day.\n",
      "00:01:16,280 --> 00:01:21,280: Which do you prefer? Working late or finishing early?\n",
      "00:01:21,280 --> 00:01:31,280: I prefer finishing early. I really enjoy the morning. I love getting up early and going for a run.\n",
      "00:01:31,280 --> 00:01:37,280: There is nothing like watching the sunrise while drinking my morning coffee.\n",
      "00:01:37,280 --> 00:01:44,280: Really? I am opposite. I love sleeping in.\n",
      "00:01:44,280 --> 00:01:53,280: I am most alert in the evenings. I'm a real night owl.\n",
      "00:01:53,280 --> 00:01:59,280: Well, you know what they say. The early bird catches the worm.\n",
      "00:01:59,280 --> 00:02:08,280: You know, you could be right. Maybe I will try to go to bed a little earlier tonight.\n",
      "\n",
      "Subtitles saved to video2.srt\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extracts the audio from a video file and saves it as a separate file.\n",
    "    \"\"\"\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    video.close()\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    \"\"\"\n",
    "    Formats seconds into an SRT timestamp (HH:MM:SS,ms).\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{milliseconds:03}\"\n",
    "\n",
    "def generate_subtitles(video_path, model_path, processor_path, whisper_model_name=\"base\"):\n",
    "    \"\"\"\n",
    "    Generates subtitles for a video using the fine-tuned Wav2Vec2 model for transcription and the Whisper model for correction.\n",
    "    \"\"\"\n",
    "    # Load Whisper model for correction\n",
    "    try:\n",
    "        whisper_model = whisper.load_model(whisper_model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Whisper model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Load the fine-tuned Wav2Vec2 model and processor\n",
    "    asr_pipeline = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_path,\n",
    "        tokenizer=processor_path\n",
    "    )\n",
    "\n",
    "    # Extract audio from video\n",
    "    audio_path = \"temp_audio.wav\"\n",
    "    extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "    # Step 1: Transcribe audio with Wav2Vec2 model\n",
    "    print(\"Transcribing audio using self-trained Wav2Vec2 model...\")\n",
    "    result_wav2vec = asr_pipeline(audio_path)\n",
    "    wav2vec_transcript = result_wav2vec['text'].lower()  # Convert transcript to lowercase\n",
    "\n",
    "    # Display the transcript from the self-trained Wav2Vec2 model\n",
    "    print(\"\\nTranscript from self-trained Wav2Vec2 model:\")\n",
    "    print(wav2vec_transcript)\n",
    "\n",
    "    # Step 2: Use Whisper model for better vocabulary and timestamp correction\n",
    "    print(\"\\nTranscribing audio using Whisper model for correction...\")\n",
    "    result_whisper = whisper_model.transcribe(audio_path)\n",
    "\n",
    "    # Display the corrected subtitles from Whisper\n",
    "    print(\"\\nCorrected subtitles from Whisper model:\")\n",
    "    for segment in result_whisper[\"segments\"]:\n",
    "        print(f\"{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}: {segment['text'].strip()}\")\n",
    "\n",
    "    # Create SRT file path\n",
    "    srt_path = os.path.splitext(video_path)[0] + \".srt\"\n",
    "\n",
    "    # Step 3: Generate subtitles using Whisper's improved segments\n",
    "    with open(srt_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "        for i, segment in enumerate(result_whisper[\"segments\"]):\n",
    "            # Write SRT segment using Whisper's improved timing and vocabulary\n",
    "            start = format_timestamp(segment[\"start\"])\n",
    "            end = format_timestamp(segment[\"end\"])\n",
    "            text = segment[\"text\"].strip()\n",
    "            srt_file.write(f\"{i + 1}\\n{start} --> {end}\\n{text}\\n\\n\")\n",
    "\n",
    "    print(f\"\\nSubtitles saved to {srt_path}\")\n",
    "\n",
    "    # Clean up temporary audio file\n",
    "    os.remove(audio_path)\n",
    "\n",
    "# Example usage\n",
    "video_path = \"video2.mp4\"  # Replace with your video file path\n",
    "model_path = \"E:\\\\video_player\\\\subtitle-generator\"  # Path to the fine-tuned Wav2Vec2 model\n",
    "processor_path = \"E:\\\\video_player\\\\subtitle-generator\"  # Path to the saved processor\n",
    "generate_subtitles(video_path, model_path, processor_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted video2.srt to video2.vtt\n"
     ]
    }
   ],
   "source": [
    "def convert_srt_to_vtt(srt_file_path, vtt_file_path):\n",
    "    try:\n",
    "        with open(srt_file_path, 'r', encoding='utf-8') as srt_file:\n",
    "            srt_content = srt_file.readlines()\n",
    "\n",
    "        vtt_content = ['WEBVTT\\n\\n']  # VTT files need to start with \"WEBVTT\"\n",
    "\n",
    "        for line in srt_content:\n",
    "            # Replace commas with dots in timestamps\n",
    "            if '-->' in line:\n",
    "                line = line.replace(',', '.')\n",
    "            vtt_content.append(line)\n",
    "\n",
    "        with open(vtt_file_path, 'w', encoding='utf-8') as vtt_file:\n",
    "            vtt_file.writelines(vtt_content)\n",
    "\n",
    "        print(f\"Successfully converted {srt_file_path} to {vtt_file_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: SRT file not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Usage example:\n",
    "srt_file = 'video2.srt'  # Replace with your .srt file path\n",
    "vtt_file = 'video2.vtt'  # Replace with desired .vtt file path\n",
    "convert_srt_to_vtt(srt_file, vtt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcribing audio using self-trained Wav2Vec2 model...\n",
      "\n",
      "Transcript from self-trained Wav2Vec2 model:\n",
      "you probably heard the story of how redit is selling their data to gugle for a i scraping how automatic is selling all the word present tumbler data and now how financial times to selling their data to opena i and is doesn't sound right because we've been told that these  i companies are taking old is data and basically invalidating the data sourcsis ter coming from by sharing the data so you don't get to the source so what exactly's happening here this is more complicated than it sounds you see they're two different things these a i companies can do with the data one miss use it for training in o building nou models basem adata and theyre definitely doing that in some respect but the other one is to use the data ask a grounded source and that's really interesting so thet me explain here sa very simplifid drawing of what happens when you use a a i system like chache b t you put in a prompt the prompt go to the a i system and the a i system creates a completion this is the response that comes out of the  i system the challenge with this model is if you ask the a i system something that exists with in its training data it's likely to put together something that looks like an answer thus correct but it's just putting together tokens to make up something that looks like language is not actually answering your question so there's no guarantee that the answer will be correct the way we solv this is by adding a grounded source so when you ask the a  system o question the a i system sends off the question to the data bas withinformation and then maching information gets sent back into the a i system it combines that with your request and you get a more grounded response as actually grounded in truth and this is what they're going to be doing with these different media organizations instead of the a i system just straight up trying to answer the question it'll go to a grounded source to get some information first and use that to process and answer in the same way that when you're working with chach  t if you ask it to write an article in'll writa o ka article but if you write the start of starting point of an article and ask it to help you make the language better it'll do much better job this process is called retrieval augmented generation because it retrieves information and then an augments that information at gives it back toou and doing this we can take things one step further and introduce what's called a semantic cash so that when you put in a prompt we can go over to the grounded source gets am information put the back in the a i and then when the completion comes out we'll set it into the cash so that next time we can by past the a i entirely this is the direction a is currently going and will be going in the forseeable future instead of using the a i system to try to answer to question directly have the a i system ground it self in real data and use rag retrieval augmentad generation to pulreal data ou and then augmented in its response hop that helps\n",
      "\n",
      "Transcribing audio using Whisper model for correction...\n",
      "\n",
      "Subtitles saved to video3.srt\n",
      "\n",
      "Changes saved to output.txt\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extracts the audio from a video file and saves it as a separate file.\n",
    "    \"\"\"\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    video.close()\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    \"\"\"\n",
    "    Formats seconds into an SRT timestamp (HH:MM:SS,ms).\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{milliseconds:03}\"\n",
    "\n",
    "def generate_subtitles(video_path, model_path, processor_path, whisper_model_name=\"base\"):\n",
    "    \"\"\"\n",
    "    Generates subtitles for a video using the fine-tuned Wav2Vec2 model for transcription and the Whisper model for correction.\n",
    "    \"\"\"\n",
    "    # Load Whisper model for correction\n",
    "    try:\n",
    "        whisper_model = whisper.load_model(whisper_model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Whisper model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Load the fine-tuned Wav2Vec2 model and processor\n",
    "    asr_pipeline = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_path,\n",
    "        tokenizer=processor_path\n",
    "    )\n",
    "\n",
    "    # Extract audio from video\n",
    "    audio_path = \"temp_audio.wav\"\n",
    "    extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "    # Step 1: Transcribe audio with Wav2Vec2 model\n",
    "    print(\"Transcribing audio using self-trained Wav2Vec2 model...\")\n",
    "    result_wav2vec = asr_pipeline(audio_path)\n",
    "    wav2vec_transcript = result_wav2vec['text'].lower()  # Convert transcript to lowercase\n",
    "\n",
    "    # Display the transcript from the self-trained Wav2Vec2 model\n",
    "    print(\"\\nTranscript from self-trained Wav2Vec2 model:\")\n",
    "    print(wav2vec_transcript)\n",
    "\n",
    "    # Step 2: Use Whisper model for better vocabulary and timestamp correction\n",
    "    print(\"\\nTranscribing audio using Whisper model for correction...\")\n",
    "    result_whisper = whisper_model.transcribe(audio_path)\n",
    "\n",
    "    # Create a list to track changes made by Whisper model\n",
    "    changes = []\n",
    "    \n",
    "    # Create SRT file path\n",
    "    srt_path = os.path.splitext(video_path)[0] + \".srt\"\n",
    "    output_txt_path = \"output.txt\"\n",
    "\n",
    "    # Step 3: Generate subtitles using Whisper's improved segments\n",
    "    with open(srt_path, \"w\", encoding=\"utf-8\") as srt_file, open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        for i, segment in enumerate(result_whisper[\"segments\"]):\n",
    "            start = format_timestamp(segment['start'])\n",
    "            end = format_timestamp(segment['end'])\n",
    "            whisper_text = segment[\"text\"].strip()\n",
    "\n",
    "            # Aligning both transcriptions for output\n",
    "            start_idx = int(segment['start'] * 1000)  # Convert to milliseconds\n",
    "            end_idx = int(segment['end'] * 1000)  # Convert to milliseconds\n",
    "            wav2vec_segment = wav2vec_transcript[start_idx:end_idx].strip()\n",
    "\n",
    "            # Check for changes and save them\n",
    "            if wav2vec_segment != whisper_text:\n",
    "                changes.append(f\"Original: '{wav2vec_segment}' --> Corrected: '{whisper_text}'\")\n",
    "            \n",
    "            # Write SRT segment using Whisper's improved timing and vocabulary\n",
    "            srt_file.write(f\"{i + 1}\\n{start} --> {end}\\n{whisper_text}\\n\\n\")\n",
    "        \n",
    "        # Save the changes made by Whisper to the text file\n",
    "        txt_file.write(\"Changes made by Whisper model:\\n\")\n",
    "        for change in changes:\n",
    "            txt_file.write(change + \"\\n\")\n",
    "\n",
    "    print(f\"\\nSubtitles saved to {srt_path}\")\n",
    "    print(f\"\\nChanges saved to {output_txt_path}\")\n",
    "\n",
    "    # Clean up temporary audio file\n",
    "    os.remove(audio_path)\n",
    "\n",
    "# Example usage\n",
    "video_path = \"video3.mp4\"  # Replace with your video file path\n",
    "model_path = \"./subtitle-generator\"  # Path to the fine-tuned Wav2Vec2 model\n",
    "processor_path = \"./subtitle-generator\"  # Path to the saved processor\n",
    "generate_subtitles(video_path, model_path, processor_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\n",
      "accelerate==1.2.1\n",
      "addict==2.4.0\n",
      "aiohappyeyeballs==2.4.4\n",
      "aiohttp==3.11.11\n",
      "aiosignal==1.3.2\n",
      "annotated-types==0.7.0\n",
      "anyio==4.4.0\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "astral==3.2\n",
      "asttokens==2.4.1\n",
      "astunparse==1.6.3\n",
      "async-lru==2.0.4\n",
      "attrs==23.2.0\n",
      "audioread==3.0.1\n",
      "Babel==2.15.0\n",
      "beautifulsoup4==4.12.3\n",
      "bleach==6.1.0\n",
      "cachetools==5.5.0\n",
      "certifi==2024.6.2\n",
      "cffi==1.16.0\n",
      "chardet==5.2.0\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.8\n",
      "cloudpickle==3.1.0\n",
      "cmdstanpy==1.2.4\n",
      "colorama==0.4.6\n",
      "coloredlogs==15.0.1\n",
      "comm==0.2.2\n",
      "contourpy==1.3.0\n",
      "cycler==0.12.1\n",
      "Cython==3.0.11\n",
      "datasets==3.2.0\n",
      "debugpy==1.8.1\n",
      "decorator==4.4.2\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.8\n",
      "distro==1.9.0\n",
      "docx2pdf==0.1.8\n",
      "executing==2.0.1\n",
      "faiss-cpu==1.9.0.post1\n",
      "fastjsonschema==2.19.1\n",
      "filelock==3.16.0\n",
      "flatbuffers==24.3.25\n",
      "fonttools==4.54.1\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.5.0\n",
      "fsspec==2024.9.0\n",
      "gast==0.6.0\n",
      "geopandas==1.0.1\n",
      "google-api-core==2.19.2\n",
      "google-auth==2.34.0\n",
      "google-cloud==0.34.0\n",
      "google-cloud-speech==2.27.0\n",
      "google-pasta==0.2.0\n",
      "googleapis-common-protos==1.65.0\n",
      "grpcio==1.66.1\n",
      "grpcio-status==1.66.1\n",
      "gym==0.26.2\n",
      "gym-notices==0.0.8\n",
      "h11==0.14.0\n",
      "h5py==3.12.1\n",
      "holidays==0.58\n",
      "httpcore==1.0.5\n",
      "httpx==0.27.0\n",
      "huggingface-hub==0.25.1\n",
      "humanfriendly==10.0\n",
      "idna==3.7\n",
      "imageio==2.35.1\n",
      "imageio-ffmpeg==0.5.1\n",
      "importlib_resources==6.4.5\n",
      "ipykernel==6.29.4\n",
      "ipython==8.25.0\n",
      "ipywidgets==8.1.3\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.4\n",
      "jiter==0.5.0\n",
      "jiwer==3.0.5\n",
      "joblib==1.4.2\n",
      "json5==0.9.25\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.22.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter==1.0.0\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.10.0\n",
      "jupyter-lsp==2.2.5\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "jupyter_server==2.14.1\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.2.1\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.2\n",
      "jupyterlab_widgets==3.0.11\n",
      "keras==3.7.0\n",
      "kiwisolver==1.4.7\n",
      "lazy_loader==0.4\n",
      "libclang==18.1.1\n",
      "librosa==0.10.2.post1\n",
      "llvmlite==0.43.0\n",
      "lxml==5.2.2\n",
      "Markdown==3.7\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.5\n",
      "matplotlib==3.9.2\n",
      "matplotlib-inline==0.1.7\n",
      "mdurl==0.1.2\n",
      "mistune==3.0.2\n",
      "ml-dtypes==0.4.1\n",
      "mltu==1.2.5\n",
      "more-itertools==10.5.0\n",
      "moviepy==1.0.3\n",
      "mpmath==1.3.0\n",
      "msgpack==1.1.0\n",
      "multidict==6.1.0\n",
      "multiprocess==0.70.16\n",
      "namex==0.0.8\n",
      "nbclient==0.10.0\n",
      "nbconvert==7.16.4\n",
      "nbformat==5.10.4\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.3\n",
      "notebook==7.2.0\n",
      "notebook_shim==0.2.4\n",
      "numba==0.60.0\n",
      "numpy==1.26.4\n",
      "onnxruntime==1.19.2\n",
      "openai==1.45.0\n",
      "openai-whisper==20231117\n",
      "opencv-python==4.10.0.84\n",
      "opt_einsum==3.4.0\n",
      "optree==0.12.1\n",
      "overrides==7.7.0\n",
      "packaging==24.0\n",
      "pandas==2.2.3\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.4\n",
      "patsy==0.5.6\n",
      "pillow==10.4.0\n",
      "platformdirs==4.2.2\n",
      "plotly==5.24.1\n",
      "pooch==1.8.2\n",
      "proglog==0.1.10\n",
      "prometheus_client==0.20.0\n",
      "prompt_toolkit==3.0.46\n",
      "propcache==0.2.1\n",
      "prophet==1.1.6\n",
      "proto-plus==1.24.0\n",
      "protobuf==4.25.5\n",
      "psutil==5.9.8\n",
      "pure-eval==0.2.2\n",
      "pyarrow==18.1.0\n",
      "pyasn1==0.6.1\n",
      "pyasn1_modules==0.4.1\n",
      "pycparser==2.22\n",
      "pydantic==2.9.1\n",
      "pydantic_core==2.23.3\n",
      "pydub==0.25.1\n",
      "pygame==2.6.0\n",
      "Pygments==2.18.0\n",
      "pyogrio==0.10.0\n",
      "pyparsing==3.1.4\n",
      "pyproj==3.7.0\n",
      "pyproject-toml==0.0.10\n",
      "pyreadline3==3.5.4\n",
      "pysrt==1.1.2\n",
      "python-dateutil==2.9.0.post0\n",
      "python-docx==1.1.2\n",
      "python-json-logger==2.0.7\n",
      "pytz==2024.2\n",
      "pywin32==306\n",
      "pywinpty==2.0.13\n",
      "PyYAML==6.0.1\n",
      "pyzmq==26.0.3\n",
      "qqdm==0.0.7\n",
      "qtconsole==5.5.2\n",
      "QtPy==2.4.1\n",
      "RapidFuzz==3.11.0\n",
      "referencing==0.35.1\n",
      "regex==2024.9.11\n",
      "requests==2.32.3\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.8.1\n",
      "rpds-py==0.18.1\n",
      "rsa==4.9\n",
      "safetensors==0.4.5\n",
      "scikit-base==0.8.3\n",
      "scikit-learn==1.5.2\n",
      "scipy==1.14.1\n",
      "seaborn==0.13.2\n",
      "Send2Trash==1.8.3\n",
      "setuptools==70.0.0\n",
      "shapely==2.0.6\n",
      "six==1.16.0\n",
      "sktime==0.33.1\n",
      "sniffio==1.3.1\n",
      "soundfile==0.12.1\n",
      "soupsieve==2.5\n",
      "soxr==0.5.0.post1\n",
      "SpeechRecognition==3.10.4\n",
      "stack-data==0.6.3\n",
      "stanio==0.5.1\n",
      "statsmodels==0.14.4\n",
      "stow==1.4.1\n",
      "sympy==1.13.2\n",
      "tenacity==9.0.0\n",
      "tensorboard==2.18.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.18.0\n",
      "tensorflow_intel==2.18.0\n",
      "termcolor==2.4.0\n",
      "terminado==0.18.1\n",
      "tf_keras==2.18.0\n",
      "threadpoolctl==3.5.0\n",
      "tiktoken==0.7.0\n",
      "tinycss2==1.3.0\n",
      "tokenizers==0.20.0\n",
      "toml==0.10.2\n",
      "torch==2.4.1\n",
      "torchaudio==2.4.1\n",
      "tornado==6.4\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.3\n",
      "transformers==4.45.1\n",
      "types-python-dateutil==2.9.0.20240316\n",
      "typing_extensions==4.12.1\n",
      "tzdata==2024.2\n",
      "uri-template==1.3.0\n",
      "urllib3==2.2.1\n",
      "wcwidth==0.2.13\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.8.0\n",
      "Werkzeug==3.0.4\n",
      "wheel==0.43.0\n",
      "whisper==1.1.10\n",
      "widgetsnbextension==4.0.11\n",
      "wrapt==1.16.0\n",
      "xxhash==3.5.0\n",
      "yarl==1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pip freeze requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
